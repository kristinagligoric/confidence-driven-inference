{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be271f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm, bernoulli\n",
    "import json\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from tqdm.auto import tqdm\n",
    "from plotting_utils import plot_naive_coverage, plot_eff_sample_size, plot_coverage, plot_intervals\n",
    "from utils import logistic, logistic_cov, active_logistic_pointestimate, opt_logistic_tuning, train_tree, tree_predict, inv_hessian_col_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0734b2-6568-4f98-93b8-42f422218a64",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da11179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('datasets/politeness_dataset.csv')\n",
    "data = data.sample(frac=1).reset_index(drop=True) # shuffle data\n",
    "Yhat_string = data[\"label_gpt4o\"].to_numpy()\n",
    "confidence = data[\"confidence_in_prediction_gpt-4o\"].to_numpy()\n",
    "device = 'hedge' # 'hedge' or '1pp'\n",
    "nan_indices = list(np.where(pd.isna(confidence))[0]) + list(np.where(pd.isna(Yhat_string))[0])\n",
    "good_indices = list(set(range(len(data))) - set(nan_indices))\n",
    "confidence = confidence[good_indices]\n",
    "Yhat_string = Yhat_string[good_indices]\n",
    "Y = data[\"Politeness\"].to_numpy()[good_indices]\n",
    "n = len(Y)\n",
    "dict = {\"A\" : 1, \"B\" : 0}\n",
    "Yhat = np.array([dict[Yhat_string[i]] for i in range(n)])\n",
    "if device == 'hedge':\n",
    "    X_device = data.to_numpy()[:,[4]][good_indices]\n",
    "elif device == '1pp':\n",
    "    X_device = data.to_numpy()[:,[11]][good_indices]\n",
    "X = np.column_stack((X_device, np.ones(n))).astype(float)\n",
    "confidence = confidence.reshape(len(confidence),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bde7fee-1735-442f-9b62-57845a1a9433",
   "metadata": {},
   "source": [
    "## Effective sample size and coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908b5fc4-a48f-43cf-912a-37e8e217a744",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1  # desired error level for confidence interval\n",
    "fracs_human = np.linspace(0.05, 0.2, 10)  # overall sampling budget\n",
    "num_trials = 100\n",
    "\n",
    "true_pointest = logistic(X,Y)\n",
    "true_effect = true_pointest[0]  # first coordinate is effect of device\n",
    "true_Sigma = logistic_cov(true_pointest, X, Y, Yhat, np.ones(n), lam=0)\n",
    "\n",
    "num_methods = 4\n",
    "temp_df = pd.DataFrame({\n",
    "    \"lb\": np.zeros(num_methods),\n",
    "    \"ub\": np.zeros(num_methods),\n",
    "    \"interval width\": np.zeros(num_methods),\n",
    "    \"coverage\": np.zeros(num_methods),\n",
    "    \"estimator\": [\"\"] * num_methods,\n",
    "    \"$n_{\\mathrm{human}}$\": np.zeros(num_methods),\n",
    "    \"$n_{\\mathrm{effective}}$\": np.zeros(num_methods)\n",
    "})\n",
    "\n",
    "\n",
    "burnin_steps = 50  # we collect the first burnin_steps points to initialize sampling rule\n",
    "retrain_steps = 200  # every retrain_steps we retrain mapping from confidence to sampling probability\n",
    "tau = 0.1  # parameter for mixing with uniform sampling for increased stability\n",
    "\n",
    "# compute column of inverse Hessian with LLM labels for approximating optimal sampling rule\n",
    "h = inv_hessian_col_imputed(X, Yhat)\n",
    "\n",
    "results = []\n",
    "\n",
    "for j in tqdm(range(len(fracs_human))):\n",
    "    frac_human = fracs_human[j]\n",
    "    frac_human_adjusted = (frac_human*n - burnin_steps)/(n - burnin_steps) # remove burnin_steps samples from available budget\n",
    "    \n",
    "    for i in tqdm(range(num_trials)):\n",
    "        tree = train_tree(confidence[:burnin_steps], ((Y - Yhat)[:burnin_steps])**2)\n",
    "        uncertainties = np.sqrt(tree_predict(tree, confidence)) * np.abs(X.dot(h))\n",
    "        avg_uncertainty = np.mean(uncertainties)\n",
    "        weights_active = np.zeros(n)\n",
    "        weights_active[:burnin_steps] = 1\n",
    "        \n",
    "        for t in range(burnin_steps, n):\n",
    "            \n",
    "            if ((t-burnin_steps) % retrain_steps == 0):\n",
    "                obs_inds = np.where(weights_active)\n",
    "                tree = train_tree(confidence[obs_inds], ((Y - Yhat)[obs_inds])**2)\n",
    "                uncertainties = np.sqrt(tree_predict(tree, confidence)) * np.abs(X.dot(h))\n",
    "                avg_uncertainty = np.mean(uncertainties)\n",
    "\n",
    "            sampling_prob = uncertainties[t]/avg_uncertainty*frac_human_adjusted\n",
    "            sampling_prob = np.clip((1-tau)*sampling_prob + tau*frac_human_adjusted, 0, 1)\n",
    "            weights_active[t] = bernoulli.rvs(sampling_prob)/sampling_prob\n",
    "            \n",
    "        pointest_init = active_logistic_pointestimate(X, Y, Yhat, weights_active, lam=1)\n",
    "        lam = opt_logistic_tuning(pointest_init, X, Y, Yhat, weights_active)\n",
    "        pointest = active_logistic_pointestimate(X, Y, Yhat, weights_active, lam=lam)\n",
    "        Sigmahat = logistic_cov(pointest, X, Y, Yhat, weights_active, lam=lam)\n",
    "        l = pointest - norm.ppf(1-alpha/2)*np.sqrt(np.diag(Sigmahat)/n)\n",
    "        u = pointest + norm.ppf(1-alpha/2)*np.sqrt(np.diag(Sigmahat)/n)\n",
    "        coverage = (true_effect >= l[0])*(true_effect <= u[0])\n",
    "        temp_df.loc[0] = l[0], u[0], u[0]-l[0], coverage, \"confidence-driven\", int(n*frac_human), (true_Sigma[0,0]/Sigmahat[0,0])*n\n",
    "        \n",
    "        xi_unif = bernoulli.rvs([frac_human] * n)\n",
    "        pointest = active_logistic_pointestimate(X, Y, Yhat, xi_unif/frac_human, lam=1)\n",
    "        Sigmahat = logistic_cov(pointest, X, Y, Yhat, xi_unif/frac_human, lam=1)\n",
    "        l = pointest - norm.ppf(1-alpha/2)*np.sqrt(np.diag(Sigmahat)/n)\n",
    "        u = pointest + norm.ppf(1-alpha/2)*np.sqrt(np.diag(Sigmahat)/n)\n",
    "        coverage = (true_effect >= l[0])*(true_effect <= u[0])\n",
    "        temp_df.loc[1] = l[0], u[0], u[0]-l[0], coverage, \"human + LLM (non-adaptive)\", int(n*frac_human), (true_Sigma[0,0]/Sigmahat[0,0])*n\n",
    "        \n",
    "        pointest = logistic(X[np.where(xi_unif)], Y[np.where(xi_unif)])\n",
    "        Sigmahat = logistic_cov(pointest, X, Y, Yhat, xi_unif/frac_human, lam=0)\n",
    "        l = pointest - norm.ppf(1-alpha/2)*np.sqrt(np.diag(Sigmahat)/n)\n",
    "        u = pointest + norm.ppf(1-alpha/2)*np.sqrt(np.diag(Sigmahat)/n)\n",
    "        coverage = (true_effect >= l[0])*(true_effect <= u[0])\n",
    "        temp_df.loc[2] = l[0], u[0], u[0]-l[0], coverage, \"human only\", int(n*frac_human), (true_Sigma[0,0]/Sigmahat[0,0])*n\n",
    "\n",
    "        LLM_only_sample = np.random.choice(n, n, replace=True)\n",
    "        pointest = logistic(X[LLM_only_sample], Yhat[LLM_only_sample])\n",
    "        Sigmahat = logistic_cov(pointest, X[LLM_only_sample], Yhat[LLM_only_sample], Yhat[LLM_only_sample], np.ones(n), lam=0)\n",
    "        l = pointest - norm.ppf(1-alpha/2)*np.sqrt(np.diag(Sigmahat)/n)\n",
    "        u = pointest + norm.ppf(1-alpha/2)*np.sqrt(np.diag(Sigmahat)/n)\n",
    "        coverage = (true_effect >= l[0])*(true_effect <= u[0])\n",
    "        temp_df.loc[3] = l[0], u[0], u[0]-l[0], coverage, \"LLM only\", int(n*frac_human), 0\n",
    "\n",
    "        results += [temp_df.copy()]\n",
    "df = pd.concat(results,ignore_index=True)\n",
    "df[\"coverage\"] = df[\"coverage\"].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab64116-9833-4fc5-88ff-bd3ac652b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_eff_sample_size(df, \"plots/\" + device + \"_ESS.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8942252e-af5a-4e4a-a195-d15a2997ebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coverage(df, alpha, \"plots/\" + device + \"_coverage.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5a705d-1284-4a5f-9642-3252704c29f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_intervals(df, true_effect, num_trials, \"logistic regression coefficient Î²$_{\\mathrm{\" + device + \"}}$\", \"plots/\" + device + \"_intervals.pdf\", n_ind=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f97b45-c06e-4c8e-9cd4-08828b02018a",
   "metadata": {},
   "source": [
    "## Experiment at fixed $n_{\\text{human}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ee801-dd5b-413c-be80-df480ea0f2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1  # desired error level for confidence interval\n",
    "num_trials = 100\n",
    "nhuman = 500\n",
    "frac_human = nhuman/n\n",
    "\n",
    "true_pointest = logistic(X,Y)\n",
    "true_effect = true_pointest[0]  # first coordinate is for effect of marker\n",
    "true_Sigma = logistic_cov(true_pointest, X, Y, Yhat, np.ones(n), lam=0)\n",
    "\n",
    "num_methods = 3\n",
    "temp_df = pd.DataFrame({\n",
    "    \"estimator\": [\"\"] * num_methods,\n",
    "    \"coverage\": np.zeros(num_methods),\n",
    "    \"$n_{\\mathrm{effective}}$\": np.zeros(num_methods)\n",
    "})\n",
    "\n",
    "burnin_steps = 50  # we collect the first burnin_steps points to initialize sampling rule\n",
    "retrain_steps = 200  # every retrain_steps we retrain mapping from confidence to sampling probability\n",
    "tau = 0.1  # parameter for mixing with uniform sampling for increased stability\n",
    "\n",
    "# compute column of inverse Hessian with LLM labels for approximating optimal sampling rule\n",
    "h = inv_hessian_col_imputed(X, Yhat)\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "frac_human_adjusted = (nhuman - burnin_steps)/(n - burnin_steps) # remove burnin_steps samples from available budget\n",
    "\n",
    "for i in tqdm(range(num_trials)):\n",
    "    tree = train_tree(confidence[:burnin_steps], ((Y - Yhat)[:burnin_steps])**2)\n",
    "    uncertainties = np.sqrt(tree_predict(tree,confidence)) * np.abs(X.dot(h))\n",
    "    avg_uncertainty = np.mean(uncertainties)\n",
    "    weights_active = np.zeros(n)\n",
    "    weights_active[:burnin_steps] = 1\n",
    "    \n",
    "    for t in range(burnin_steps, n):\n",
    "        \n",
    "        if ((t-burnin_steps) % retrain_steps == 0):\n",
    "            obs_inds = np.where(weights_active)\n",
    "            tree = train_tree(confidence[obs_inds], ((Y - Yhat)[obs_inds])**2)\n",
    "            uncertainties = np.sqrt(tree_predict(tree, confidence)) * np.abs(X.dot(h))\n",
    "            avg_uncertainty = np.mean(uncertainties)\n",
    "\n",
    "        sampling_prob = uncertainties[t]/avg_uncertainty*frac_human_adjusted\n",
    "        sampling_prob = np.clip((1-tau)*sampling_prob + tau*frac_human_adjusted, 0, 1)\n",
    "        weights_active[t] = bernoulli.rvs(sampling_prob)/sampling_prob\n",
    "\n",
    "        \n",
    "    pointest_init = active_logistic_pointestimate(X, Y, Yhat, weights_active, lam=1)\n",
    "    lam = opt_logistic_tuning(pointest_init, X, Y, Yhat, weights_active)\n",
    "    pointest = active_logistic_pointestimate(X, Y, Yhat, weights_active, lam=lam)\n",
    "    Sigmahat = logistic_cov(pointest, X, Y, Yhat, weights_active, lam=lam)\n",
    "    l = pointest - norm.ppf(1-alpha/2)*np.sqrt(np.diag(Sigmahat)/n)\n",
    "    u = pointest + norm.ppf(1-alpha/2)*np.sqrt(np.diag(Sigmahat)/n)\n",
    "    coverage = (true_effect >= l[0])*(true_effect <= u[0])\n",
    "    temp_df.loc[0] =  \"confidence-driven\", coverage, (true_Sigma[0,0]/Sigmahat[0,0])*n\n",
    "    \n",
    "    xi_unif = bernoulli.rvs([frac_human] * n)\n",
    "    pointest = active_logistic_pointestimate(X, Y, Yhat, xi_unif/frac_human, lam=1)\n",
    "    Sigmahat = logistic_cov(pointest, X, Y, Yhat, xi_unif/frac_human, lam=1)\n",
    "    l = pointest - norm.ppf(1-alpha/2)*np.sqrt(np.diag(Sigmahat)/n)\n",
    "    u = pointest + norm.ppf(1-alpha/2)*np.sqrt(np.diag(Sigmahat)/n)\n",
    "    coverage = (true_effect >= l[0])*(true_effect <= u[0])\n",
    "    temp_df.loc[1] = \"human + LLM (non-adaptive)\", coverage, (true_Sigma[0,0]/Sigmahat[0,0])*n\n",
    "\n",
    "    LLM_only_sample = np.random.choice(n, n, replace=True)\n",
    "    pointest = logistic(X[LLM_only_sample], Yhat[LLM_only_sample])\n",
    "    Sigmahat = logistic_cov(pointest, X[LLM_only_sample], Yhat[LLM_only_sample], Yhat[LLM_only_sample], np.ones(n), lam=0)\n",
    "    l = pointest - norm.ppf(1-alpha/2)*np.sqrt(np.diag(Sigmahat)/n)\n",
    "    u = pointest + norm.ppf(1-alpha/2)*np.sqrt(np.diag(Sigmahat)/n)\n",
    "    coverage = (true_effect >= l[0])*(true_effect <= u[0])\n",
    "    temp_df.loc[2] = \"LLM only\", coverage, 0\n",
    "\n",
    "    results += [temp_df.copy()]\n",
    "df = pd.concat(results,ignore_index=True)\n",
    "df[\"ESS gain\"] = df[\"$n_{\\mathrm{effective}}$\"]/nhuman - 1\n",
    "df[\"coverage\"] = df[\"coverage\"].astype(bool)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
