{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be271f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm, bernoulli\n",
    "import json\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from tqdm.auto import tqdm\n",
    "from plotting_utils import plot_naive_coverage, plot_eff_sample_size, plot_coverage, plot_intervals\n",
    "from utils import train_tree, tree_predict, opt_mean_tuning, mean_ESS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0734b2-6568-4f98-93b8-42f422218a64",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da11179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('datasets/bias_dataset.csv')\n",
    "data = data.sample(frac=1).reset_index(drop=True) # shuffle data\n",
    "leaning = 'left' # 'left' or 'right'\n",
    "Yhat_string = data[\"label_gpt4o\"].to_numpy()\n",
    "Y_string = data[\"bias_text\"].to_numpy()\n",
    "confidence = data[\"confidence_in_prediction_gpt-4o\"].to_numpy()\n",
    "nan_indices = list(np.where(pd.isna(confidence))[0]) + list(np.where(pd.isna(Yhat_string))[0])\n",
    "good_indices = list(set(range(len(data))) - set(nan_indices))\n",
    "confidence = confidence[good_indices]\n",
    "Yhat_string = Yhat_string[good_indices]\n",
    "Y_string = Y_string[good_indices]\n",
    "n = len(Yhat_string)\n",
    "if leaning=='left':\n",
    "    dict = {\"A\" : 1, \"B\" : 0, \"C\" : 0, \"left\" : 1, \"center\": 0, \"right\": 0}\n",
    "elif leaning=='right':\n",
    "    dict = {\"A\" : 0, \"B\" : 0, \"C\" : 1, \"left\" : 0, \"center\": 0, \"right\": 1}\n",
    "Yhat = np.array([dict[Yhat_string[i]] for i in range(n)])\n",
    "Y = np.array([dict[Y_string[i]] for i in range(n)])\n",
    "confidence = confidence.reshape(len(confidence),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bde7fee-1735-442f-9b62-57845a1a9433",
   "metadata": {},
   "source": [
    "## Effective sample size and coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908b5fc4-a48f-43cf-912a-37e8e217a744",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1  # desired error level for confidence interval\n",
    "fracs_human = np.linspace(0.2, 0.6, 10)  # overall sampling budget\n",
    "num_trials = 100\n",
    "\n",
    "true_prevalence = np.mean(Y)\n",
    "\n",
    "num_methods = 4\n",
    "temp_df = pd.DataFrame({\n",
    "    \"lb\": np.zeros(num_methods),\n",
    "    \"ub\": np.zeros(num_methods),\n",
    "    \"interval width\": np.zeros(num_methods),\n",
    "    \"coverage\": np.zeros(num_methods),\n",
    "    \"estimator\": [\"\"] * num_methods,\n",
    "    \"$n_{\\mathrm{human}}$\": np.zeros(num_methods),\n",
    "    \"$n_{\\mathrm{effective}}$\": np.zeros(num_methods)\n",
    "})\n",
    "\n",
    "\n",
    "burnin_steps = 50  # we collect the first burnin_steps points to initialize sampling rule\n",
    "retrain_steps = 50  # every retrain_steps we retrain mapping from confidence to sampling probability\n",
    "tau = 0.1  # parameter for mixing with uniform sampling for increased stability\n",
    "\n",
    "results = []\n",
    "\n",
    "for j in tqdm(range(len(fracs_human))):\n",
    "    frac_human = fracs_human[j]\n",
    "    frac_human_adjusted = (frac_human*n - burnin_steps)/(n - burnin_steps) # remove burnin_steps samples from available budget\n",
    "    \n",
    "    for i in tqdm(range(num_trials)):\n",
    "        tree = train_tree(confidence[:burnin_steps], ((Y - Yhat)[:burnin_steps])**2)\n",
    "        uncertainties = np.sqrt(tree_predict(tree, confidence))\n",
    "        avg_uncertainty = np.mean(uncertainties)\n",
    "        weights_active = np.zeros(n)\n",
    "        weights_active[:burnin_steps] = 1\n",
    "        sampling_ratio = np.zeros(n)\n",
    "        \n",
    "        for t in range(burnin_steps, n):\n",
    "\n",
    "            if ((t-burnin_steps) % retrain_steps == 0):\n",
    "                obs_inds = np.where(weights_active)\n",
    "                tree = train_tree(confidence[obs_inds], ((Y - Yhat)[obs_inds])**2)\n",
    "                uncertainties = np.sqrt(tree_predict(tree, confidence))\n",
    "                avg_uncertainty = np.mean(uncertainties)\n",
    "\n",
    "            sampling_prob = uncertainties[t]/avg_uncertainty*frac_human_adjusted\n",
    "            sampling_prob = np.clip((1-tau)*sampling_prob + tau*frac_human_adjusted, 0, 1)\n",
    "            sampling_ratio[t] = (1-sampling_prob)/sampling_prob\n",
    "\n",
    "            weights_active[t] = bernoulli.rvs(sampling_prob)/sampling_prob\n",
    "\n",
    "        lam = opt_mean_tuning(Y, Yhat, weights_active, sampling_ratio)\n",
    "        pointest = np.mean(lam*Yhat + (Y-lam*Yhat)*weights_active)\n",
    "        varhat = np.var(lam*Yhat + (Y-lam*Yhat)*weights_active)\n",
    "        l = pointest - norm.ppf(1-alpha/2)*np.sqrt(varhat/n)\n",
    "        u = pointest + norm.ppf(1-alpha/2)*np.sqrt(varhat/n)\n",
    "        coverage = (true_prevalence >= l)*(true_prevalence <= u)\n",
    "        temp_df.loc[0] = l, u, u-l, coverage, \"confidence-driven\", int(n*frac_human), mean_ESS(Y, varhat)\n",
    "        \n",
    "        xi_unif = bernoulli.rvs([frac_human] * n)\n",
    "        pointest = np.mean(Yhat + (Y-Yhat)*xi_unif/frac_human)\n",
    "        varhat = np.var(Yhat + (Y-Yhat)*xi_unif/frac_human)\n",
    "        l = pointest - norm.ppf(1-alpha/2)*np.sqrt(varhat/n)\n",
    "        u = pointest + norm.ppf(1-alpha/2)*np.sqrt(varhat/n)\n",
    "        coverage = (true_prevalence >= l)*(true_prevalence <= u)\n",
    "        temp_df.loc[1] = l, u, u-l, coverage, \"human + LLM (non-adaptive)\", int(n*frac_human), mean_ESS(Y, varhat)\n",
    "\n",
    "        pointest = np.mean(Y*xi_unif/frac_human)\n",
    "        varhat = np.var(Y*xi_unif/frac_human)\n",
    "        l = pointest - norm.ppf(1-alpha/2)*np.sqrt(varhat/n)\n",
    "        u = pointest + norm.ppf(1-alpha/2)*np.sqrt(varhat/n)\n",
    "        coverage = (true_prevalence >= l)*(true_prevalence <= u)\n",
    "        temp_df.loc[2] = l, u, u-l, coverage, \"human only\", int(n*frac_human), mean_ESS(Y, varhat)\n",
    "\n",
    "        LLM_only_sample = np.random.choice(n, n, replace=True)\n",
    "        pointest = np.mean(Yhat[LLM_only_sample])\n",
    "        varhat = np.var(Yhat[LLM_only_sample])\n",
    "        l = pointest - norm.ppf(1-alpha/2)*np.sqrt(varhat/n)\n",
    "        u = pointest + norm.ppf(1-alpha/2)*np.sqrt(varhat/n)\n",
    "        coverage = (true_prevalence >= l)*(true_prevalence <= u)\n",
    "        temp_df.loc[3] = l, u, u-l, coverage, \"LLM only\", int(n*frac_human), 0\n",
    "        \n",
    "        results += [temp_df.copy()]\n",
    "df = pd.concat(results,ignore_index=True)\n",
    "df[\"coverage\"] = df[\"coverage\"].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab64116-9833-4fc5-88ff-bd3ac652b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_eff_sample_size(df, \"plots/\" + leaning + \"_leaning_ESS.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8942252e-af5a-4e4a-a195-d15a2997ebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coverage(df, alpha, \"plots/\" + leaning + \"_leaning_coverage.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5a705d-1284-4a5f-9642-3252704c29f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_intervals(df, true_prevalence, num_trials, \"prevalence $p_{\\mathrm{\" + leaning + \"}}$\", \"plots/\" + leaning + \"_leaning_intervals.pdf\", n_ind=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f586a3c1-1193-44c3-b4b3-4f4b6b076050",
   "metadata": {},
   "source": [
    "## Experiment at fixed $n_{\\text{human}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710560ac-128a-4e23-a30c-87e3c204aff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1  # desired error level for confidence interval\n",
    "num_trials = 100\n",
    "nhuman = 500\n",
    "frac_human = nhuman/n\n",
    "\n",
    "true_prevalence = np.mean(Y)\n",
    "\n",
    "num_methods = 3\n",
    "temp_df = pd.DataFrame({\n",
    "    \"estimator\": [\"\"] * num_methods,\n",
    "    \"coverage\": np.zeros(num_methods),\n",
    "    \"$n_{\\mathrm{effective}}$\": np.zeros(num_methods)\n",
    "})\n",
    "\n",
    "\n",
    "burnin_steps = 50  # we collect the first burnin_steps points to initialize sampling rule\n",
    "retrain_steps = 50  # every retrain_steps we retrain mapping from confidence to sampling probability\n",
    "tau = 0.1  # parameter for mixing with uniform sampling for increased stability\n",
    "\n",
    "results = []\n",
    "\n",
    "frac_human_adjusted = (nhuman - burnin_steps)/(n - burnin_steps) # remove burnin_steps samples from available budget\n",
    "\n",
    "for i in tqdm(range(num_trials)):\n",
    "    tree = train_tree(confidence[:burnin_steps], ((Y - Yhat)[:burnin_steps])**2)\n",
    "    uncertainties = np.sqrt(tree_predict(tree, confidence))\n",
    "    avg_uncertainty = np.mean(uncertainties)\n",
    "    weights_active = np.zeros(n)\n",
    "    weights_active[:burnin_steps] = 1\n",
    "    sampling_ratio = np.zeros(n)\n",
    "    \n",
    "    for t in range(burnin_steps, n):\n",
    "\n",
    "        if ((t-burnin_steps) % retrain_steps == 0):\n",
    "            obs_inds = np.where(weights_active)\n",
    "            tree = train_tree(confidence[obs_inds], ((Y - Yhat)[obs_inds])**2)\n",
    "            uncertainties = np.sqrt(tree_predict(tree, confidence))\n",
    "            avg_uncertainty = np.mean(uncertainties)\n",
    "\n",
    "        sampling_prob = uncertainties[t]/avg_uncertainty*frac_human_adjusted\n",
    "        sampling_prob = np.clip((1-tau)*sampling_prob + tau*frac_human_adjusted, 0, 1)\n",
    "        sampling_ratio[t] = (1-sampling_prob)/sampling_prob\n",
    "\n",
    "        weights_active[t] = bernoulli.rvs(sampling_prob)/sampling_prob\n",
    "\n",
    "    lam = opt_mean_tuning(Y, Yhat, weights_active, sampling_ratio)\n",
    "    pointest = np.mean(lam*Yhat + (Y-lam*Yhat)*weights_active)\n",
    "    varhat = np.var(lam*Yhat + (Y-lam*Yhat)*weights_active)\n",
    "    l = pointest - norm.ppf(1-alpha/2)*np.sqrt(varhat/n)\n",
    "    u = pointest + norm.ppf(1-alpha/2)*np.sqrt(varhat/n)\n",
    "    coverage = (true_prevalence >= l)*(true_prevalence <= u)\n",
    "    temp_df.loc[0] = \"confidence-driven\", coverage, mean_ESS(Y, varhat)\n",
    "    \n",
    "    xi_unif = bernoulli.rvs([frac_human] * n)\n",
    "    pointest = np.mean(Yhat + (Y-Yhat)*xi_unif/frac_human)\n",
    "    varhat = np.var(Yhat + (Y-Yhat)*xi_unif/frac_human)\n",
    "    l = pointest - norm.ppf(1-alpha/2)*np.sqrt(varhat/n)\n",
    "    u = pointest + norm.ppf(1-alpha/2)*np.sqrt(varhat/n)\n",
    "    coverage = (true_prevalence >= l)*(true_prevalence <= u)\n",
    "    temp_df.loc[1] = \"human + LLM (non-adaptive)\", coverage, mean_ESS(Y, varhat)\n",
    "\n",
    "    LLM_only_sample = np.random.choice(n, n, replace=True)\n",
    "    pointest = np.mean(Yhat[LLM_only_sample])\n",
    "    varhat = np.var(Yhat[LLM_only_sample])\n",
    "    l = pointest - norm.ppf(1-alpha/2)*np.sqrt(varhat/n)\n",
    "    u = pointest + norm.ppf(1-alpha/2)*np.sqrt(varhat/n)\n",
    "    coverage = (true_prevalence >= l)*(true_prevalence <= u)\n",
    "    temp_df.loc[2] = \"LLM only\", coverage, 0\n",
    "    \n",
    "    results += [temp_df.copy()]\n",
    "df = pd.concat(results,ignore_index=True)\n",
    "df[\"ESS gain\"] = df[\"$n_{\\mathrm{effective}}$\"]/nhuman - 1\n",
    "df[\"coverage\"] = df[\"coverage\"].astype(bool)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
