{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be271f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm, bernoulli\n",
    "import json\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from tqdm.auto import tqdm\n",
    "from plotting_utils import plot_naive_coverage, plot_eff_sample_size, plot_coverage, plot_intervals\n",
    "from utils import odds_ratio_ci, train_tree, tree_predict, opt_mean_tuning\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0734b2-6568-4f98-93b8-42f422218a64",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8938a23f-b66c-492d-ac37-b6d7b7332ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('datasets/stance_dataset.csv')\n",
    "data = data.sample(frac=1).reset_index(drop=True) # shuffle data\n",
    "\n",
    "affirming_devices = ['uncover', 'realize', 'know', 'understand', 'learn', 'concede',\n",
    "'remember', 'recall', 'discover', 'show', 'reveal', 'see',\n",
    "'forget', 'find', 'point out', 'indicate', 'acknowledge',\n",
    "'admit', 'realize', 'notice', 'certify', 'verify', 'corroborate', 'affirm', 'confirm', 'agree', 'conclude',\n",
    "'proven', 'settled', 'conclusive', 'definitive',\n",
    "'famed', 'unequivocal', 'skilful', 'notable', 'strong', 'famous', 'Nobel', 'skillful',\n",
    "'Nobelist', 'Nobel Laureate', 'Nobel prize winner',\n",
    "'Nobel prize winning', 'prize winning', 'award',\n",
    "'winning', 'distinguished', 'well-grounded', 'esteemed', 'proficient', 'key', 'evidence', 'noted', 'top',\n",
    "'preeminent', 'breakthrough', 'significant', 'intelligent', 'of import', 'celebrated', 'novel', 'recent',\n",
    "'major', 'landmark', 'important', 'distinguished',\n",
    "'renowned', 'peer-reviewed', 'expert', 'leading',\n",
    "'thousand', '1000', 'hundred', '100', 'unanimous', 'diverse',\n",
    "'substantial', 'many', 'multiple', 'dozen', 'numerous']\n",
    "\n",
    "escaped_terms = [re.escape(term) for term in affirming_devices]\n",
    "pattern = '|'.join(escaped_terms)\n",
    "data['contains_affirming_device'] = data['sentence'].str.contains(pattern, case=False, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da11179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yhat_string = data[\"label_gpt4o\"].to_numpy()\n",
    "confidence = data[\"confidence_in_prediction_gpt-4o\"].to_numpy()\n",
    "nan_indices = list(np.where(pd.isna(confidence))[0]) + list(np.where(pd.isna(Yhat_string))[0])\n",
    "good_indices = list(set(range(len(data))) - set(nan_indices))\n",
    "confidence = confidence[good_indices]\n",
    "device = data['contains_affirming_device'].to_numpy()[good_indices]\n",
    "Yhat_string = Yhat_string[good_indices]\n",
    "Y_string = data[\"MACE_pred\"].to_numpy()[good_indices]\n",
    "n = len(Yhat_string)\n",
    "dict = {\"A\" : 1, \"B\" : 0, \"C\" : 0, \"agrees\": 1, \"neutral\" : 0, \"disagrees\": 0}\n",
    "Yhat = np.array([dict[Yhat_string[i]] for i in range(n)])\n",
    "Y = np.array([dict[Y_string[i]] for i in range(n)])\n",
    "confidence = confidence.reshape(len(confidence),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5206b3df-ea0e-4ee2-b88d-59a1d7c5e94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1 = Y[device]\n",
    "Y0 = Y[~device]\n",
    "Yhat1 = Yhat[device]\n",
    "Yhat0 = Yhat[~device]\n",
    "n0 = len(Y0)\n",
    "n1 = len(Y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bde7fee-1735-442f-9b62-57845a1a9433",
   "metadata": {},
   "source": [
    "## Effective sample size and coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908b5fc4-a48f-43cf-912a-37e8e217a744",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1  # desired error level for confidence interval\n",
    "fracs_human = np.linspace(0.2, 0.5, 10)  # overall sampling budget\n",
    "num_trials = 100\n",
    "\n",
    "mu0 = Y0.mean()\n",
    "mu1 = Y1.mean()\n",
    "true_odds_ratio = (mu1 / (1 - mu1)) / (mu0 / (1 - mu0))\n",
    "true_var = (1/np.sum(Y0==0) + 1/np.sum(Y0==1) + 1/np.sum(Y1==0) + 1/np.sum(Y1==1))*n\n",
    "\n",
    "num_methods = 4\n",
    "temp_df = pd.DataFrame({\n",
    "    \"lb\": np.zeros(num_methods),\n",
    "    \"ub\": np.zeros(num_methods),\n",
    "    \"interval width\": np.zeros(num_methods),\n",
    "    \"coverage\": np.zeros(num_methods),\n",
    "    \"estimator\": [\"\"] * num_methods,\n",
    "    \"$n_{\\mathrm{human}}$\": np.zeros(num_methods),\n",
    "    \"$n_{\\mathrm{effective}}$\": np.zeros(num_methods)\n",
    "})\n",
    "\n",
    "burnin_steps = 50  # we collect the first burnin_steps points to initialize sampling rule\n",
    "retrain_steps = 50  # every retrain_steps we retrain mapping from confidence to sampling probability\n",
    "tau = 0.1  # parameter for mixing with uniform sampling for increased stability\n",
    "\n",
    "results = []\n",
    "\n",
    "for j in tqdm(range(len(fracs_human))):\n",
    "    frac_human = fracs_human[j]\n",
    "    frac_human_adjusted = (frac_human*n - burnin_steps)/(n - burnin_steps) # remove burnin_steps samples from available budget for both classes\n",
    "    \n",
    "    for i in tqdm(range(num_trials)):\n",
    "        tree = train_tree(confidence[:burnin_steps], ((Y - Yhat)[:burnin_steps])**2)\n",
    "        uncertainties = np.sqrt(tree_predict(tree, confidence))\n",
    "        avg_uncertainty = np.mean(uncertainties)\n",
    "        weights_active = np.zeros(n)\n",
    "        sampling_ratio = np.zeros(n)\n",
    "        weights_active[:burnin_steps] = 1\n",
    "        \n",
    "        for t in range(burnin_steps, n):\n",
    "            if ((t-burnin_steps) % retrain_steps == 0):\n",
    "                obs_inds = np.where(weights_active)\n",
    "                tree = train_tree(confidence[obs_inds], ((Y - Yhat)[obs_inds])**2)\n",
    "                uncertainties = np.sqrt(tree_predict(tree, confidence))\n",
    "                avg_uncertainty = np.mean(uncertainties)\n",
    "\n",
    "            sampling_prob = uncertainties[t]/avg_uncertainty*frac_human_adjusted\n",
    "            sampling_prob = np.clip((1-tau)*sampling_prob + tau*frac_human_adjusted, 0, 1)\n",
    "            sampling_ratio[t] = (1-sampling_prob)/sampling_prob\n",
    "            weights_active[t] = bernoulli.rvs(sampling_prob)/sampling_prob\n",
    "            \n",
    "        weights_active0 = weights_active[~device]\n",
    "        weights_active1 = weights_active[device]\n",
    "        sampling_ratio0 = sampling_ratio[~device]\n",
    "        sampling_ratio1 = sampling_ratio[device]\n",
    "\n",
    "        lam0 = opt_mean_tuning(Y0, Yhat0, weights_active0, sampling_ratio0)\n",
    "        lam1 = opt_mean_tuning(Y1, Yhat1, weights_active1, sampling_ratio1)\n",
    "        l, u, varhat = odds_ratio_ci(Y0, Yhat0, Y1, Yhat1, weights_active0, weights_active1, alpha, lhat0=lam0, lhat1=lam1)\n",
    "        coverage = (true_odds_ratio >= l)*(true_odds_ratio <= u)\n",
    "        temp_df.loc[0] = l, u, u-l, coverage, \"confidence-driven\", int(n*frac_human), (true_var/varhat)*n\n",
    "\n",
    "        xi_unif0 = bernoulli.rvs([frac_human] * n0)\n",
    "        xi_unif1 = bernoulli.rvs([frac_human] * n1)\n",
    "        l, u, varhat = odds_ratio_ci(Y0, Yhat0, Y1, Yhat1, xi_unif0/frac_human, xi_unif1/frac_human, alpha, lhat0=1, lhat1=1)\n",
    "        coverage = (true_odds_ratio >= l)*(true_odds_ratio <= u)\n",
    "        temp_df.loc[1] = l, u, u-l, coverage, \"human + LLM (non-adaptive)\", int(n*frac_human), (true_var/varhat)*n\n",
    "        \n",
    "        mu0 = Y0[np.where(xi_unif0)].mean()\n",
    "        mu1 = Y1[np.where(xi_unif1)].mean()\n",
    "        odds_ratio_est = np.log((mu1 / (1 - mu1)) / (mu0 / (1 - mu0)))\n",
    "        varhat = (1/np.sum(Y0[np.where(xi_unif0)]==0) + 1/np.sum(Y0[np.where(xi_unif0)]==1) + 1/np.sum(Y1[np.where(xi_unif1)]==0) + 1/np.sum(Y1[np.where(xi_unif1)]==1))\n",
    "        l = np.exp(odds_ratio_est - norm.ppf(1-alpha/2)*np.sqrt(varhat))\n",
    "        u = np.exp(odds_ratio_est + norm.ppf(1-alpha/2)*np.sqrt(varhat))\n",
    "        coverage = (true_odds_ratio >= l)*(true_odds_ratio <= u)\n",
    "        temp_df.loc[2] = l, u, u-l, coverage, \"human only\", int(n*frac_human), (true_var/varhat)\n",
    "\n",
    "        LLM_only_sample0 = np.random.choice(n0, n0, replace=True)\n",
    "        LLM_only_sample1 = np.random.choice(n1, n1, replace=True)\n",
    "        Yhat0_i = Yhat0[LLM_only_sample0]\n",
    "        Yhat1_i = Yhat1[LLM_only_sample1]\n",
    "        muhat0 = Yhat0_i.mean()\n",
    "        muhat1 = Yhat1_i.mean()\n",
    "        odds_ratio_est = (muhat1 / (1 - muhat1)) / (muhat0 / (1 - muhat0))\n",
    "        varhat = (1/np.sum(Yhat0_i==0) + 1/np.sum(Yhat0_i==1) + 1/np.sum(Yhat1_i==0) + 1/np.sum(Yhat1_i==1))\n",
    "        l = np.exp(odds_ratio_est - norm.ppf(1-alpha/2)*np.sqrt(varhat))\n",
    "        u = np.exp(odds_ratio_est + norm.ppf(1-alpha/2)*np.sqrt(varhat))\n",
    "        coverage = (true_odds_ratio >= l)*(true_odds_ratio <= u)\n",
    "        temp_df.loc[3] = l, u, u-l, coverage, \"LLM only\", int(n*frac_human), 0\n",
    "        \n",
    "        results += [temp_df.copy()]\n",
    "df = pd.concat(results,ignore_index=True)\n",
    "df[\"coverage\"] = df[\"coverage\"].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab64116-9833-4fc5-88ff-bd3ac652b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_eff_sample_size(df, \"plots/stance_ESS.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8942252e-af5a-4e4a-a195-d15a2997ebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coverage(df, alpha, \"plots/stance_coverage.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5a705d-1284-4a5f-9642-3252704c29f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_intervals(df, true_odds_ratio, num_trials, \"odds ratio $O_{\\mathrm{agreement}}$\", \"plots/stance_intervals.pdf\", n_ind=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2481ab-a967-449c-b92c-e2f50c89c6f8",
   "metadata": {},
   "source": [
    "## Experiment at fixed $n_{\\text{human}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f1bc5f-1bf9-4714-9896-239d812fdc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1  # desired error level for confidence interval\n",
    "num_trials = 100\n",
    "nhuman = 500\n",
    "frac_human = nhuman/n\n",
    "\n",
    "mu0 = Y0.mean()\n",
    "mu1 = Y1.mean()\n",
    "true_odds_ratio = (mu1 / (1 - mu1)) / (mu0 / (1 - mu0))\n",
    "true_var = (1/np.sum(Y0==0) + 1/np.sum(Y0==1) + 1/np.sum(Y1==0) + 1/np.sum(Y1==1))*n\n",
    "\n",
    "columns = [\"estimator\", \"coverage\", \"effective sample size\"]\n",
    "num_methods = 3\n",
    "temp_df = pd.DataFrame({\n",
    "    \"estimator\": [\"\"] * num_methods,\n",
    "    \"coverage\": np.zeros(num_methods),\n",
    "    \"$n_{\\mathrm{effective}}$\": np.zeros(num_methods)\n",
    "})\n",
    "\n",
    "\n",
    "burnin_steps = 50  # we collect the first burnin_steps points to initialize sampling rule\n",
    "retrain_steps = 50  # every retrain_steps we retrain mapping from confidence to sampling probability\n",
    "tau = 0.1  # parameter for mixing with uniform sampling for increased stability\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "frac_human_adjusted = (nhuman - burnin_steps)/(n - burnin_steps) # remove burnin_steps samples from available budget for both classes\n",
    "\n",
    "for i in tqdm(range(num_trials)):\n",
    "    tree = train_tree(confidence[:burnin_steps], ((Y - Yhat)[:burnin_steps])**2)\n",
    "    uncertainties = np.sqrt(tree_predict(tree, confidence))\n",
    "    avg_uncertainty = np.mean(uncertainties)\n",
    "    weights_active = np.zeros(n)\n",
    "    sampling_ratio = np.zeros(n)\n",
    "    weights_active[:burnin_steps] = 1\n",
    "    \n",
    "    for t in range(burnin_steps, n):\n",
    "        if ((t-burnin_steps) % retrain_steps == 0):\n",
    "            obs_inds = np.where(weights_active)\n",
    "            tree = train_tree(confidence[obs_inds], ((Y - Yhat)[obs_inds])**2)\n",
    "            uncertainties = np.sqrt(tree_predict(tree, confidence))\n",
    "            avg_uncertainty = np.mean(uncertainties)\n",
    "\n",
    "        sampling_prob = uncertainties[t]/avg_uncertainty*frac_human_adjusted\n",
    "        sampling_prob = np.clip((1-tau)*sampling_prob + tau*frac_human_adjusted, 0, 1)\n",
    "        sampling_ratio[t] = (1-sampling_prob)/sampling_prob\n",
    "\n",
    "        weights_active[t] = bernoulli.rvs(sampling_prob)/sampling_prob\n",
    "        \n",
    "    weights_active0 = weights_active[~device]\n",
    "    weights_active1 = weights_active[device]\n",
    "    sampling_ratio0 = sampling_ratio[~device]\n",
    "    sampling_ratio1 = sampling_ratio[device]\n",
    "\n",
    "    lam0 = opt_mean_tuning(Y0, Yhat0, weights_active0, sampling_ratio0)\n",
    "    lam1 = opt_mean_tuning(Y1, Yhat1, weights_active1, sampling_ratio1)\n",
    "    l, u, varhat = odds_ratio_ci(Y0, Yhat0, Y1, Yhat1, weights_active0, weights_active1, alpha, lhat0=lam0, lhat1=lam1)\n",
    "    coverage = (true_odds_ratio >= l)*(true_odds_ratio <= u)\n",
    "    temp_df.loc[0] = \"confidence-driven\", coverage, (true_var/varhat)*n\n",
    "\n",
    "    xi_unif0 = bernoulli.rvs([frac_human] * n0)\n",
    "    xi_unif1 = bernoulli.rvs([frac_human] * n1)\n",
    "    l, u, varhat = odds_ratio_ci(Y0, Yhat0, Y1, Yhat1, xi_unif0/frac_human, xi_unif1/frac_human, alpha, lhat0=1, lhat1=1)\n",
    "    coverage = (true_odds_ratio >= l)*(true_odds_ratio <= u)\n",
    "    temp_df.loc[1] = \"human + LLM (non-adaptive)\", coverage, (true_var/varhat)*n\n",
    "\n",
    "    LLM_only_sample0 = np.random.choice(n0, n0, replace=True)\n",
    "    LLM_only_sample1 = np.random.choice(n1, n1, replace=True)\n",
    "    Yhat0_i = Yhat0[LLM_only_sample0]\n",
    "    Yhat1_i = Yhat1[LLM_only_sample1]\n",
    "    muhat0 = Yhat0_i.mean()\n",
    "    muhat1 = Yhat1_i.mean()\n",
    "    odds_ratio_est = (muhat1 / (1 - muhat1)) / (muhat0 / (1 - muhat0))\n",
    "    varhat = (1/np.sum(Yhat0_i==0) + 1/np.sum(Yhat0_i==1) + 1/np.sum(Yhat1_i==0) + 1/np.sum(Yhat1_i==1))\n",
    "    l = np.exp(odds_ratio_est - norm.ppf(1-alpha/2)*np.sqrt(varhat))\n",
    "    u = np.exp(odds_ratio_est + norm.ppf(1-alpha/2)*np.sqrt(varhat))\n",
    "    coverage = (true_odds_ratio >= l)*(true_odds_ratio <= u)\n",
    "    temp_df.loc[2] = \"LLM only\", coverage, 0\n",
    "    \n",
    "    results += [temp_df.copy()]\n",
    "df = pd.concat(results,ignore_index=True)\n",
    "df[\"ESS gain\"] = df[\"$n_{\\mathrm{effective}}$\"]/nhuman - 1\n",
    "df[\"coverage\"] = df[\"coverage\"].astype(bool)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
